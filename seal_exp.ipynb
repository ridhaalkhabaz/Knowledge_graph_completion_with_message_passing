{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa90dbba",
   "metadata": {},
   "source": [
    "## Knowledge graph experiments using SEAL \n",
    "### Author: Ridha Alkhabaz \n",
    "\n",
    "We are applying the seal paradigm onto FB15k-237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "856e1861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import os, sys\n",
    "import torch\n",
    "import pdb\n",
    "import copy as cp\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import scipy.sparse as ssp\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from tqdm import tqdm\n",
    "from shutil import copy\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_sparse import coalesce\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.datasets import FB15k_237\n",
    "from torch_geometric.data import Data, Dataset, InMemoryDataset, DataLoader\n",
    "from torch_geometric.utils import to_networkx, to_undirected\n",
    "from torch_geometric.datasets import RelLinkPredDataset\n",
    "from utils import *\n",
    "from torch.nn import (ModuleList, Linear, Conv1d, MaxPool1d, Embedding, ReLU, \n",
    "                      Sequential, BatchNorm1d as BN)\n",
    "from torch_geometric.nn import (GCNConv, SAGEConv, GINConv, \n",
    "                                global_sort_pool, global_add_pool, global_mean_pool)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d253339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpful funcstions and classes \n",
    "# this is for model definition and other things \n",
    "class DGCNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_layers, max_z, k=0.6, train_dataset=None, \n",
    "                 dynamic_train=False, GNN=GCNConv, use_feature=False, \n",
    "                 node_embedding=None):\n",
    "        super(DGCNN, self).__init__()\n",
    "\n",
    "        self.use_feature = use_feature\n",
    "        self.node_embedding = node_embedding\n",
    "\n",
    "        if k <= 1:  # Transform percentile to number.\n",
    "            if train_dataset is None:\n",
    "                k = 30\n",
    "            else:\n",
    "                if dynamic_train:\n",
    "                    sampled_train = train_dataset[:1000]\n",
    "                else:\n",
    "                    sampled_train = train_dataset\n",
    "                num_nodes = sorted([g.num_nodes for g in sampled_train])\n",
    "                k = num_nodes[int(math.ceil(k * len(num_nodes))) - 1]\n",
    "                k = max(10, k)\n",
    "        self.k = int(k)\n",
    "\n",
    "        self.max_z = max_z\n",
    "        self.z_embedding = Embedding(self.max_z, hidden_channels)\n",
    "\n",
    "        self.convs = ModuleList()\n",
    "        initial_channels = hidden_channels\n",
    "        if self.use_feature:\n",
    "            initial_channels += train_dataset.num_features\n",
    "        if self.node_embedding is not None:\n",
    "            initial_channels += node_embedding.embedding_dim\n",
    "\n",
    "        self.convs.append(GNN(initial_channels, hidden_channels))\n",
    "        for i in range(0, num_layers-1):\n",
    "            self.convs.append(GNN(hidden_channels, hidden_channels))\n",
    "        self.convs.append(GNN(hidden_channels, 1))\n",
    "\n",
    "        conv1d_channels = [16, 32]\n",
    "        total_latent_dim = hidden_channels * num_layers + 1\n",
    "        conv1d_kws = [total_latent_dim, 5]\n",
    "        self.conv1 = Conv1d(1, conv1d_channels[0], conv1d_kws[0],\n",
    "                            conv1d_kws[0])\n",
    "        self.maxpool1d = MaxPool1d(2, 2)\n",
    "        self.conv2 = Conv1d(conv1d_channels[0], conv1d_channels[1],\n",
    "                            conv1d_kws[1], 1)\n",
    "        dense_dim = int((self.k - 2) / 2 + 1)\n",
    "        dense_dim = (dense_dim - conv1d_kws[1] + 1) * conv1d_channels[1]\n",
    "        self.lin1 = Linear(dense_dim, 128)\n",
    "        self.lin2 = Linear(128, 1)\n",
    "\n",
    "    def forward(self, z, edge_index, batch, x=None, edge_weight=None, node_id=None):\n",
    "        z_emb = self.z_embedding(z)\n",
    "        if z_emb.ndim == 3:  # in case z has multiple integer labels\n",
    "            z_emb = z_emb.sum(dim=1)\n",
    "        if self.use_feature and x is not None:\n",
    "            x = torch.cat([z_emb, x.to(torch.float)], 1)\n",
    "        else:\n",
    "            x = z_emb\n",
    "        if self.node_embedding is not None and node_id is not None:\n",
    "            n_emb = self.node_embedding(node_id)\n",
    "            x = torch.cat([x, n_emb], 1)\n",
    "        xs = [x]\n",
    "\n",
    "        for conv in self.convs:\n",
    "            xs += [torch.tanh(conv(xs[-1], edge_index, edge_weight))]\n",
    "        x = torch.cat(xs[1:], dim=-1)\n",
    "\n",
    "        # Global pooling.\n",
    "        x = global_sort_pool(x, batch, self.k)\n",
    "        x = x.unsqueeze(1)  # [num_graphs, 1, k * hidden]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool1d(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # [num_graphs, dense_dim]\n",
    "\n",
    "        # MLP.\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "# this is for dataset definition\n",
    "class SEALDataset(InMemoryDataset):\n",
    "    def __init__(self, root, data, split_edge, num_hops, percent=100, split='train', \n",
    "                 use_coalesce=False, node_label='drnl', ratio_per_hop=1.0, \n",
    "                 max_nodes_per_hop=None, directed=False):\n",
    "        self.data = data\n",
    "        self.split_edge = split_edge\n",
    "        self.num_hops = num_hops\n",
    "        self.percent = int(percent) if percent >= 1.0 else percent\n",
    "        self.split = split\n",
    "        self.use_coalesce = use_coalesce\n",
    "        self.node_label = node_label\n",
    "        self.ratio_per_hop = ratio_per_hop\n",
    "        self.max_nodes_per_hop = max_nodes_per_hop\n",
    "        self.directed = directed\n",
    "        super(SEALDataset, self).__init__(root)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        if self.percent == 100:\n",
    "            name = 'SEAL_{}_data'.format(self.split)\n",
    "        else:\n",
    "            name = 'SEAL_{}_data_{}'.format(self.split, self.percent)\n",
    "        name += '.pt'\n",
    "        return [name]\n",
    "\n",
    "    def process(self):\n",
    "        pos_edge, neg_edge = get_pos_neg_edges(self.split, self.split_edge, \n",
    "                                               self.data.edge_index, \n",
    "                                               self.data.num_nodes, \n",
    "                                               self.percent)\n",
    "\n",
    "        if self.use_coalesce:  # compress mutli-edge into edge with weight\n",
    "            self.data.edge_index, self.data.edge_weight = coalesce(\n",
    "                self.data.edge_index, self.data.edge_weight, \n",
    "                self.data.num_nodes, self.data.num_nodes)\n",
    "\n",
    "        if 'edge_weight' in self.data:\n",
    "            edge_weight = self.data.edge_weight.view(-1)\n",
    "        else:\n",
    "            edge_weight = torch.ones(self.data.edge_index.size(1), dtype=int)\n",
    "        A = ssp.csr_matrix(\n",
    "            (edge_weight, (self.data.edge_index[0], self.data.edge_index[1])), \n",
    "            shape=(self.data.num_nodes, self.data.num_nodes)\n",
    "        )\n",
    "\n",
    "        if self.directed:\n",
    "            A_csc = A.tocsc()\n",
    "        else:\n",
    "            A_csc = None\n",
    "        \n",
    "        # Extract enclosing subgraphs for pos and neg edges\n",
    "        pos_list = extract_enclosing_subgraphs(\n",
    "            pos_edge, A, self.data.x, 1, self.num_hops, self.node_label, \n",
    "            self.ratio_per_hop, self.max_nodes_per_hop, self.directed, A_csc)\n",
    "        neg_list = extract_enclosing_subgraphs(\n",
    "            neg_edge, A, self.data.x, 0, self.num_hops, self.node_label, \n",
    "            self.ratio_per_hop, self.max_nodes_per_hop, self.directed, A_csc)\n",
    "\n",
    "        torch.save(self.collate(pos_list + neg_list), self.processed_paths[0])\n",
    "        del pos_list, neg_list\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(train_loader, ncols=70)\n",
    "    for data in pbar:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        x =  None\n",
    "        edge_weight = None\n",
    "        node_id = data.node_id if emb else None\n",
    "        logits = model(data.z, data.edge_index, data.batch, x, edge_weight, node_id)\n",
    "        loss = BCEWithLogitsLoss()(logits.view(-1), data.y.to(torch.float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "\n",
    "    return total_loss / len(train_dataset)\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    y_pred, y_true = [], []\n",
    "    for data in tqdm(val_loader, ncols=70):\n",
    "        data = data.to(device)\n",
    "        x =  None\n",
    "        edge_weight = None\n",
    "        node_id = data.node_id if emb else None\n",
    "        logits = model(data.z, data.edge_index, data.batch, x, edge_weight, node_id)\n",
    "        y_pred.append(logits.view(-1).cpu())\n",
    "        y_true.append(data.y.view(-1).cpu().to(torch.float))\n",
    "    val_pred, val_true = torch.cat(y_pred), torch.cat(y_true)\n",
    "    pos_val_pred = val_pred[val_true==1]\n",
    "    neg_val_pred = val_pred[val_true==0]\n",
    "\n",
    "    y_pred, y_true = [], []\n",
    "    for data in tqdm(test_loader, ncols=70):\n",
    "        data = data.to(device)\n",
    "        x =  None\n",
    "        edge_weight =  None\n",
    "        node_id = data.node_id if emb else None\n",
    "        logits = model(data.z, data.edge_index, data.batch, x, edge_weight, node_id)\n",
    "        y_pred.append(logits.view(-1).cpu())\n",
    "        y_true.append(data.y.view(-1).cpu().to(torch.float))\n",
    "    test_pred, test_true = torch.cat(y_pred), torch.cat(y_true)\n",
    "    pos_test_pred = test_pred[test_true==1]\n",
    "    neg_test_pred = test_pred[test_true==0]\n",
    "    \n",
    "   \n",
    "    results = evaluate_mrr(val_pred, val_true, test_pred, test_true)\n",
    "\n",
    "    return results\n",
    "def evaluate_mrr(pos_val_pred, neg_val_pred, pos_test_pred, neg_test_pred):\n",
    "    pos_val_pred = pos_val_pred.view(-1, 1)\n",
    "    neg_val_pred = neg_val_pred.view(-1, 1)\n",
    "    pos_test_pred = pos_test_pred.view(-1, 1)\n",
    "    neg_test_pred = neg_test_pred.view(-1, 1)\n",
    "    optimistic_rank_test = (neg_test_pred > pos_test_pred).sum(dim=1)\n",
    "    optimistic_rank_val = (neg_val_pred > pos_val_pred).sum(dim=1)\n",
    "    pessimistic_rank_test = (neg_test_pred >= pos_test_pred).sum(dim=1)\n",
    "    pessimistic_rank_val = (neg_val_pred >= pos_val_pred).sum(dim=1)\n",
    "    ranking_list_test = 0.5 * (optimistic_rank_test + pessimistic_rank_test) + 1\n",
    "    ranking_list_val = 0.5 * (optimistic_rank_val + pessimistic_rank_val) + 1\n",
    "\n",
    "    results = {}\n",
    "    valid_mrr =  (1./ranking_list_val.to(torch.float)).mean()\n",
    "    test_mrr = (1./ranking_list_test.to(torch.float)).mean()\n",
    "   \n",
    "    results['MRR'] = (valid_mrr, test_mrr)\n",
    "    return results\n",
    "def do_edge_split_v2(data, fast_split=False, val_ratio=0.05, test_ratio=0.1):\n",
    "    random.seed(234)\n",
    "    torch.manual_seed(234)\n",
    "    # print('check oone')\n",
    "    if not fast_split:\n",
    "        # print('check two')\n",
    "        data = train_test_split_edges(data, val_ratio, test_ratio)\n",
    "        # print('thre')\n",
    "        edge_index, _ = add_self_loops(data.train_pos_edge_index)\n",
    "        # print('four')\n",
    "        data.train_neg_edge_index = negative_sampling(\n",
    "            edge_index, num_nodes=data.num_nodes,\n",
    "            num_neg_samples=data.train_pos_edge_index.size(1))\n",
    "    else:\n",
    "        # print('check ttwo')\n",
    "        num_nodes = data.num_nodes\n",
    "        row, col = data.edge_index\n",
    "    \n",
    "        # print('check thre')\n",
    "        # print(row, col)\n",
    "        # Return upper triangular portion.\n",
    "        mask = row < col\n",
    "        # print(mask.shape)\n",
    "        row, col = row[mask], col[mask]\n",
    "        n_v = int(math.floor(val_ratio * row.size(0)))\n",
    "        n_t = int(math.floor(test_ratio * row.size(0)))\n",
    "        # Positive edges.\n",
    "        \n",
    "        perm = torch.randperm(row.size(0))\n",
    "        row, col = row[perm], col[perm]\n",
    "        r, c = row[:n_v], col[:n_v]\n",
    "        data.val_pos_edge_index = torch.stack([r, c], dim=0)\n",
    "        r, c = row[n_v:n_v + n_t], col[n_v:n_v + n_t]\n",
    "        data.test_pos_edge_index = torch.stack([r, c], dim=0)\n",
    "        r, c = row[n_v + n_t:], col[n_v + n_t:]\n",
    "        data.train_pos_edge_index = torch.stack([r, c], dim=0)\n",
    "        # Negative edges (cannot guarantee (i,j) and (j,i) won't both appear)\n",
    "        neg_edge_index = negative_sampling(\n",
    "            data.edge_index, num_nodes=num_nodes,\n",
    "            num_neg_samples=row.size(0))\n",
    "        data.val_neg_edge_index = neg_edge_index[:, :n_v]\n",
    "        data.test_neg_edge_index = neg_edge_index[:, n_v:n_v + n_t]\n",
    "        data.train_neg_edge_index = neg_edge_index[:, n_v + n_t:]\n",
    "\n",
    "    split_edge = {'train': {}, 'valid': {}, 'test': {}}\n",
    "    split_edge['train']['edge'] = data.train_pos_edge_index.t()\n",
    "    split_edge['train']['edge_neg'] = data.train_neg_edge_index.t()\n",
    "    split_edge['valid']['edge'] = data.val_pos_edge_index.t()\n",
    "    split_edge['valid']['edge_neg'] = data.val_neg_edge_index.t()\n",
    "    split_edge['test']['edge'] = data.test_pos_edge_index.t()\n",
    "    split_edge['test']['edge_neg'] = data.test_neg_edge_index.t()\n",
    "    return split_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b227f717-7635-4c2d-8a70-9ce86693ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = FB15k_237('data')\n",
    "data_train = dataset_train[0]\n",
    "dataset_val = FB15k_237('data', split='val')\n",
    "data_val = dataset_val[0]\n",
    "dataset_test = FB15k_237('data', split='test')\n",
    "data_test = dataset_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43891ad1-260f-4197-980f-54e80edfbc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()\n",
    "data.num_nodes = 14541\n",
    "data.edge_type = torch.cat((data_train.edge_type, data_val.edge_type, data_test.edge_type),0)\n",
    "data.edge_index = torch.cat((data_train.edge_index, data_val.edge_index, data_test.edge_index),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdef1b8c-4092-47b9-a752-56d4bba1745c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(num_nodes=14541, edge_type=[310116], val_pos_edge_index=[2, 6226], test_pos_edge_index=[2, 12452], train_pos_edge_index=[2, 196914], train_neg_adj_mask=[14541, 14541], val_neg_edge_index=[2, 6226], test_neg_edge_index=[2, 12452], train_neg_edge_index=[2, 196914], edge_index=[2, 196914])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_edge = do_edge_split_v2(data)\n",
    "data.edge_index = split_edge['train']['edge'].t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7f332f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = dataset_train.root\n",
    "train_dataset = eval('SEALDataset')(\n",
    "    path, \n",
    "    data, \n",
    "    split_edge, \n",
    "    num_hops=1, \n",
    "    percent=100, \n",
    "    split='train', \n",
    "    use_coalesce=False, \n",
    "    node_label='drnl', \n",
    "    ratio_per_hop=1.0, \n",
    "    max_nodes_per_hop=None, \n",
    "    directed=True, \n",
    ")\n",
    "val_dataset = eval('SEALDataset')(\n",
    "    path, \n",
    "    data, \n",
    "    split_edge, \n",
    "    num_hops=1, \n",
    "    percent=100, \n",
    "    split='valid', \n",
    "    use_coalesce=False, \n",
    "    node_label='drnl', \n",
    "    ratio_per_hop=1.0, \n",
    "    max_nodes_per_hop=None, \n",
    "    directed=True, \n",
    ") \n",
    "test_dataset = eval('SEALDataset')(\n",
    "    path, \n",
    "    data, \n",
    "    split_edge, \n",
    "    num_hops=1, \n",
    "    percent=100, \n",
    "    split='test', \n",
    "    use_coalesce=False, \n",
    "    node_label='drnl', \n",
    "    ratio_per_hop=1.0, \n",
    "    max_nodes_per_hop=None, \n",
    "    directed=True, \n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47fe3793",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_z = 1000 \n",
    "train_loader = DataLoader(train_dataset, batch_size=32, \n",
    "                          shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, \n",
    "                           num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, \n",
    "                           num_workers=0)\n",
    "model = DGCNN(hidden_channels=32, num_layers=3, max_z=max_z, k=0.6, \n",
    "                      train_dataset=train_dataset, use_feature=None, \n",
    "                      node_embedding=None).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "770cf3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters is 101058\n"
     ]
    }
   ],
   "source": [
    "parameters = list(model.parameters())\n",
    "optimizer = torch.optim.Adam(params=parameters, lr=0.0001)\n",
    "total_params = sum(p.numel() for param in parameters for p in param)\n",
    "print(f'Total number of parameters is {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9c53c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 10817/10817 [03:16<00:00, 55.00it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:23<00:00, 53.18it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:17<00:00, 54.79it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:18<00:00, 54.38it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:11<00:00, 56.53it/s]\n",
      "100%|███████████████████████████████| 345/345 [00:04<00:00, 85.61it/s]\n",
      "100%|███████████████████████████████| 691/691 [00:08<00:00, 80.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR\n",
      "Run: 02, Epoch: 05, Loss: 0.2803, Valid: 72.08%, Test: 71.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 10817/10817 [03:19<00:00, 54.28it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:12<00:00, 56.24it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:15<00:00, 55.37it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:15<00:00, 55.33it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:16<00:00, 55.15it/s]\n",
      "100%|██████████████████████████████| 345/345 [00:03<00:00, 109.65it/s]\n",
      "100%|██████████████████████████████| 691/691 [00:06<00:00, 113.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR\n",
      "Run: 02, Epoch: 10, Loss: 0.2764, Valid: 71.89%, Test: 71.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 10817/10817 [03:16<00:00, 55.01it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:28<00:00, 51.82it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:35<00:00, 50.24it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:23<00:00, 53.09it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:18<00:00, 54.55it/s]\n",
      "100%|██████████████████████████████| 345/345 [00:03<00:00, 107.63it/s]\n",
      "100%|██████████████████████████████| 691/691 [00:06<00:00, 109.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR\n",
      "Run: 02, Epoch: 15, Loss: 0.2745, Valid: 72.68%, Test: 72.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 10817/10817 [03:15<00:00, 55.38it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:16<00:00, 55.04it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:16<00:00, 54.96it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:16<00:00, 55.05it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:16<00:00, 55.02it/s]\n",
      "100%|██████████████████████████████| 345/345 [00:03<00:00, 106.86it/s]\n",
      "100%|██████████████████████████████| 691/691 [00:06<00:00, 105.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR\n",
      "Run: 02, Epoch: 20, Loss: 0.2732, Valid: 72.26%, Test: 72.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 10817/10817 [03:16<00:00, 55.18it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:14<00:00, 55.50it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:17<00:00, 54.82it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:17<00:00, 54.79it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:17<00:00, 54.74it/s]\n",
      "100%|██████████████████████████████| 345/345 [00:03<00:00, 104.63it/s]\n",
      "100%|██████████████████████████████| 691/691 [00:06<00:00, 109.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR\n",
      "Run: 02, Epoch: 25, Loss: 0.2732, Valid: 71.76%, Test: 71.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 10817/10817 [03:21<00:00, 53.76it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:16<00:00, 55.18it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:17<00:00, 54.66it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:17<00:00, 54.76it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:18<00:00, 54.48it/s]\n",
      "100%|██████████████████████████████| 345/345 [00:03<00:00, 107.14it/s]\n",
      "100%|██████████████████████████████| 691/691 [00:06<00:00, 108.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR\n",
      "Run: 02, Epoch: 30, Loss: 0.2713, Valid: 72.08%, Test: 71.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "emb = None\n",
    "start_epoch = 1\n",
    "loggers = {}\n",
    "run = 1\n",
    "for epoch in range(start_epoch, start_epoch + 30):\n",
    "        loss = train()\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            results = test()\n",
    "            # for key, result in results.items():\n",
    "            #     loggers[key].add_result(run, result)\n",
    "\n",
    "            # if epoch % 1 == 0:\n",
    "                # model_name = os.path.join(\n",
    "                #     args.res_dir, 'run{}_model_checkpoint{}.pth'.format(run+1, epoch))\n",
    "                # optimizer_name = os.path.join(\n",
    "                #     args.res_dir, 'run{}_optimizer_checkpoint{}.pth'.format(run+1, epoch))\n",
    "                # torch.save(model.state_dict(), model_name)\n",
    "                # torch.save(optimizer.state_dict(), optimizer_name)\n",
    "\n",
    "            for key, result in results.items():\n",
    "                valid_res, test_res = result\n",
    "                to_print = (f'Run: {run + 1:02d}, Epoch: {epoch:02d}, ' +\n",
    "                            f'Loss: {loss:.4f}, Valid: {100 * valid_res:.2f}%, ' +\n",
    "                            f'Test: {100 * test_res:.2f}%')\n",
    "                print(key)\n",
    "                print(to_print)\n",
    "                    # with open(log_file, 'a') as f:\n",
    "                    #     print(key, file=f)\n",
    "                    #     print(to_print, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20dfc2be-cf34-4796-972d-465e0801068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './mods/seal_fbk_epch30.pt')\n",
    "torch.save(optimizer.state_dict(), './mods/seal_fbk_optim_epch30.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a527ca44-eeb6-4299-8438-6eba684a4db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 10817/10817 [10:09<00:00, 17.74it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:33<00:00, 50.71it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:50<00:00, 46.97it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:47<00:00, 47.45it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:39<00:00, 49.37it/s]\n",
      "100%|██████████████████████████████| 345/345 [00:03<00:00, 102.82it/s]\n",
      "100%|██████████████████████████████| 691/691 [00:06<00:00, 108.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR\n",
      "Run: 03, Epoch: 35, Loss: 0.2702, Valid: 71.08%, Test: 70.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 10817/10817 [03:26<00:00, 52.41it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:19<00:00, 54.11it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:24<00:00, 52.81it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:21<00:00, 53.71it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:21<00:00, 53.66it/s]\n",
      "100%|██████████████████████████████| 345/345 [00:03<00:00, 106.05it/s]\n",
      "100%|██████████████████████████████| 691/691 [00:06<00:00, 108.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR\n",
      "Run: 03, Epoch: 40, Loss: 0.2694, Valid: 71.92%, Test: 71.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 10817/10817 [03:21<00:00, 53.69it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:19<00:00, 54.21it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:19<00:00, 54.20it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:20<00:00, 53.95it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:20<00:00, 53.84it/s]\n",
      "100%|██████████████████████████████| 345/345 [00:03<00:00, 106.25it/s]\n",
      "100%|██████████████████████████████| 691/691 [00:06<00:00, 110.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR\n",
      "Run: 03, Epoch: 45, Loss: 0.2689, Valid: 72.58%, Test: 72.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 10817/10817 [03:20<00:00, 53.90it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:22<00:00, 53.35it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:22<00:00, 53.43it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:22<00:00, 53.47it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:23<00:00, 53.13it/s]\n",
      "100%|██████████████████████████████| 345/345 [00:03<00:00, 105.28it/s]\n",
      "100%|██████████████████████████████| 691/691 [00:06<00:00, 106.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR\n",
      "Run: 03, Epoch: 50, Loss: 0.2692, Valid: 71.75%, Test: 71.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 10817/10817 [03:21<00:00, 53.73it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:20<00:00, 53.84it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:22<00:00, 53.46it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:23<00:00, 53.17it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:23<00:00, 53.08it/s]\n",
      "100%|██████████████████████████████| 345/345 [00:03<00:00, 105.22it/s]\n",
      "100%|██████████████████████████████| 691/691 [00:06<00:00, 105.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR\n",
      "Run: 03, Epoch: 55, Loss: 0.2684, Valid: 72.14%, Test: 71.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 10817/10817 [03:23<00:00, 53.08it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:22<00:00, 53.40it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:22<00:00, 53.44it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:21<00:00, 53.58it/s]\n",
      "100%|███████████████████████████| 10817/10817 [04:38<00:00, 38.83it/s]\n",
      "100%|██████████████████████████████| 345/345 [00:03<00:00, 105.91it/s]\n",
      "100%|██████████████████████████████| 691/691 [00:06<00:00, 107.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR\n",
      "Run: 03, Epoch: 60, Loss: 0.2692, Valid: 72.01%, Test: 71.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 10817/10817 [03:31<00:00, 51.10it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:32<00:00, 50.97it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:31<00:00, 51.22it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:25<00:00, 52.76it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:25<00:00, 52.76it/s]\n",
      "100%|██████████████████████████████| 345/345 [00:03<00:00, 104.53it/s]\n",
      "100%|██████████████████████████████| 691/691 [00:06<00:00, 108.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR\n",
      "Run: 03, Epoch: 65, Loss: 0.2684, Valid: 71.30%, Test: 71.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 10817/10817 [03:24<00:00, 52.87it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:24<00:00, 52.78it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:25<00:00, 52.55it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:24<00:00, 52.88it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:25<00:00, 52.64it/s]\n",
      "100%|██████████████████████████████| 345/345 [00:03<00:00, 104.04it/s]\n",
      "100%|██████████████████████████████| 691/691 [00:06<00:00, 107.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR\n",
      "Run: 03, Epoch: 70, Loss: 0.2683, Valid: 71.81%, Test: 71.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 10817/10817 [03:26<00:00, 52.38it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:25<00:00, 52.51it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:25<00:00, 52.57it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:26<00:00, 52.42it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:25<00:00, 52.61it/s]\n",
      "100%|██████████████████████████████| 345/345 [00:03<00:00, 103.67it/s]\n",
      "100%|██████████████████████████████| 691/691 [00:06<00:00, 106.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR\n",
      "Run: 03, Epoch: 75, Loss: 0.2674, Valid: 72.38%, Test: 72.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 10817/10817 [03:26<00:00, 52.47it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:31<00:00, 51.16it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:35<00:00, 50.13it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:34<00:00, 50.41it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:33<00:00, 50.65it/s]\n",
      "100%|██████████████████████████████| 345/345 [00:03<00:00, 101.39it/s]\n",
      "100%|██████████████████████████████| 691/691 [00:06<00:00, 102.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR\n",
      "Run: 03, Epoch: 80, Loss: 0.2672, Valid: 72.60%, Test: 72.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 10817/10817 [03:34<00:00, 50.45it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:33<00:00, 50.70it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:34<00:00, 50.46it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:33<00:00, 50.62it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:34<00:00, 50.46it/s]\n",
      "100%|██████████████████████████████| 345/345 [00:03<00:00, 102.80it/s]\n",
      "100%|██████████████████████████████| 691/691 [00:06<00:00, 102.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR\n",
      "Run: 03, Epoch: 85, Loss: 0.2693, Valid: 71.82%, Test: 71.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 10817/10817 [03:34<00:00, 50.34it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:32<00:00, 50.82it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:35<00:00, 50.10it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:35<00:00, 50.22it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:34<00:00, 50.44it/s]\n",
      "100%|███████████████████████████████| 345/345 [00:03<00:00, 97.31it/s]\n",
      "100%|██████████████████████████████| 691/691 [00:06<00:00, 104.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR\n",
      "Run: 03, Epoch: 90, Loss: 0.2685, Valid: 72.09%, Test: 71.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 10817/10817 [03:34<00:00, 50.46it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:34<00:00, 50.50it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:34<00:00, 50.47it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:35<00:00, 50.28it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:35<00:00, 50.30it/s]\n",
      "100%|███████████████████████████████| 345/345 [00:03<00:00, 99.99it/s]\n",
      "100%|██████████████████████████████| 691/691 [00:06<00:00, 102.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR\n",
      "Run: 03, Epoch: 95, Loss: 0.2677, Valid: 72.14%, Test: 71.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 10817/10817 [03:36<00:00, 49.88it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:35<00:00, 50.20it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:36<00:00, 49.98it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:37<00:00, 49.76it/s]\n",
      "100%|███████████████████████████| 10817/10817 [03:36<00:00, 50.04it/s]\n",
      "100%|██████████████████████████████| 345/345 [00:03<00:00, 100.22it/s]\n",
      "100%|██████████████████████████████| 691/691 [00:06<00:00, 101.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR\n",
      "Run: 03, Epoch: 100, Loss: 0.2663, Valid: 71.76%, Test: 71.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "emb = None\n",
    "start_epoch = 31\n",
    "loggers = {}\n",
    "run = 2\n",
    "for epoch in range(start_epoch, start_epoch + 70):\n",
    "        loss = train()\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            results = test()\n",
    "            # for key, result in results.items():\n",
    "            #     loggers[key].add_result(run, result)\n",
    "\n",
    "            # if epoch % 1 == 0:\n",
    "                # model_name = os.path.join(\n",
    "                #     args.res_dir, 'run{}_model_checkpoint{}.pth'.format(run+1, epoch))\n",
    "                # optimizer_name = os.path.join(\n",
    "                #     args.res_dir, 'run{}_optimizer_checkpoint{}.pth'.format(run+1, epoch))\n",
    "                # torch.save(model.state_dict(), model_name)\n",
    "                # torch.save(optimizer.state_dict(), optimizer_name)\n",
    "\n",
    "            for key, result in results.items():\n",
    "                valid_res, test_res = result\n",
    "                to_print = (f'Run: {run + 1:02d}, Epoch: {epoch:02d}, ' +\n",
    "                            f'Loss: {loss:.4f}, Valid: {100 * valid_res:.2f}%, ' +\n",
    "                            f'Test: {100 * test_res:.2f}%')\n",
    "                print(key)\n",
    "                print(to_print)\n",
    "                    # with open(log_file, 'a') as f:\n",
    "                    #     print(key, file=f)\n",
    "                    #     print(to_print, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2d1656d-f25a-4b30-8d72-16510ad41d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './mods/seal_fbk_epch100.pt')\n",
    "torch.save(optimizer.state_dict(), './mods/seal_fbk_optim_epch100.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b341b2a9-4858-4509-8b82-ef2f19bf46c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f0d3974e5463ee0ac00e06e495b2991e9318e837bca724d3120374d12898c6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
