{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c57a182a-e844-4125-b385-485f32d6614f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.datasets import RelLinkPredDataset\n",
    "from torch_geometric.nn import GAE, RGCNConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d67f79e9-fbb1-4066-9650-feda55dcdc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "path = 'rdata'\n",
    "dataset = RelLinkPredDataset(path, 'FB15k-237')\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "\n",
    "class RGCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, hidden_channels, num_relations):\n",
    "        super().__init__()\n",
    "        self.node_emb = Parameter(torch.empty(num_nodes, hidden_channels))\n",
    "        self.conv1 = RGCNConv(hidden_channels, hidden_channels, num_relations,\n",
    "                              num_blocks=5)\n",
    "        self.conv2 = RGCNConv(hidden_channels, hidden_channels, num_relations,\n",
    "                              num_blocks=5)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.node_emb)\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "    def forward(self, edge_index, edge_type):\n",
    "        x = self.node_emb\n",
    "        x = self.conv1(x, edge_index, edge_type).relu_()\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_type)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DistMultDecoder(torch.nn.Module):\n",
    "    def __init__(self, num_relations, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.rel_emb = Parameter(torch.empty(num_relations, hidden_channels))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.rel_emb)\n",
    "\n",
    "    def forward(self, z, edge_index, edge_type):\n",
    "        z_src, z_dst = z[edge_index[0]], z[edge_index[1]]\n",
    "        rel = self.rel_emb[edge_type]\n",
    "        return torch.sum(z_src * rel * z_dst, dim=1)\n",
    "\n",
    "\n",
    "model = GAE(\n",
    "    RGCNEncoder(data.num_nodes, 500, dataset.num_relations),\n",
    "    DistMultDecoder(dataset.num_relations // 2, 500),\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "def negative_sampling(edge_index, num_nodes):\n",
    "    # Sample edges by corrupting either the subject or the object of each edge.\n",
    "    mask_1 = torch.rand(edge_index.size(1)) < 0.5\n",
    "    mask_2 = ~mask_1\n",
    "\n",
    "    neg_edge_index = edge_index.clone()\n",
    "    neg_edge_index[0, mask_1] = torch.randint(num_nodes, (mask_1.sum(), ),\n",
    "                                              device=neg_edge_index.device)\n",
    "    neg_edge_index[1, mask_2] = torch.randint(num_nodes, (mask_2.sum(), ),\n",
    "                                              device=neg_edge_index.device)\n",
    "    return neg_edge_index\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    z = model.encode(data.edge_index, data.edge_type)\n",
    "\n",
    "    pos_out = model.decode(z, data.train_edge_index, data.train_edge_type)\n",
    "\n",
    "    neg_edge_index = negative_sampling(data.train_edge_index, data.num_nodes)\n",
    "    neg_out = model.decode(z, neg_edge_index, data.train_edge_type)\n",
    "\n",
    "    out = torch.cat([pos_out, neg_out])\n",
    "    gt = torch.cat([torch.ones_like(pos_out), torch.zeros_like(neg_out)])\n",
    "    cross_entropy_loss = F.binary_cross_entropy_with_logits(out, gt)\n",
    "    reg_loss = z.pow(2).mean() + model.decoder.rel_emb.pow(2).mean()\n",
    "    loss = cross_entropy_loss + 1e-2 * reg_loss\n",
    "\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    z = model.encode(data.edge_index, data.edge_type)\n",
    "\n",
    "    valid_mrr = compute_mrr(z, data.valid_edge_index, data.valid_edge_type)\n",
    "    test_mrr = compute_mrr(z, data.test_edge_index, data.test_edge_type)\n",
    "\n",
    "    return valid_mrr, test_mrr\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_rank(ranks):\n",
    "    # fair ranking prediction as the average\n",
    "    # of optimistic and pessimistic ranking\n",
    "    true = ranks[0]\n",
    "    optimistic = (ranks > true).sum() + 1\n",
    "    pessimistic = (ranks >= true).sum()\n",
    "    return (optimistic + pessimistic).float() * 0.5\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_mrr(z, edge_index, edge_type):\n",
    "    ranks = []\n",
    "    for i in tqdm(range(edge_type.numel())):\n",
    "        (src, dst), rel = edge_index[:, i], edge_type[i]\n",
    "\n",
    "        # Try all nodes as tails, but delete true triplets:\n",
    "        tail_mask = torch.ones(data.num_nodes, dtype=torch.bool)\n",
    "        for (heads, tails), types in [\n",
    "            (data.train_edge_index, data.train_edge_type),\n",
    "            (data.valid_edge_index, data.valid_edge_type),\n",
    "            (data.test_edge_index, data.test_edge_type),\n",
    "        ]:\n",
    "            tail_mask[tails[(heads == src) & (types == rel)]] = False\n",
    "\n",
    "        tail = torch.arange(data.num_nodes)[tail_mask]\n",
    "        tail = torch.cat([torch.tensor([dst]), tail])\n",
    "        head = torch.full_like(tail, fill_value=src)\n",
    "        eval_edge_index = torch.stack([head, tail], dim=0)\n",
    "        eval_edge_type = torch.full_like(tail, fill_value=rel)\n",
    "\n",
    "        out = model.decode(z, eval_edge_index, eval_edge_type)\n",
    "        rank = compute_rank(out)\n",
    "        ranks.append(rank)\n",
    "\n",
    "        # Try all nodes as heads, but delete true triplets:\n",
    "        head_mask = torch.ones(data.num_nodes, dtype=torch.bool)\n",
    "        for (heads, tails), types in [\n",
    "            (data.train_edge_index, data.train_edge_type),\n",
    "            (data.valid_edge_index, data.valid_edge_type),\n",
    "            (data.test_edge_index, data.test_edge_type),\n",
    "        ]:\n",
    "            head_mask[heads[(tails == dst) & (types == rel)]] = False\n",
    "\n",
    "        head = torch.arange(data.num_nodes)[head_mask]\n",
    "        head = torch.cat([torch.tensor([src]), head])\n",
    "        tail = torch.full_like(head, fill_value=dst)\n",
    "        eval_edge_index = torch.stack([head, tail], dim=0)\n",
    "        eval_edge_type = torch.full_like(head, fill_value=rel)\n",
    "\n",
    "        out = model.decode(z, eval_edge_index, eval_edge_type)\n",
    "        rank = compute_rank(out)\n",
    "        ranks.append(rank)\n",
    "\n",
    "    return (1. / torch.tensor(ranks, dtype=torch.float)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c2f39d8-d6c4-4a9d-90b4-f9a0c6155a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00051, Loss: 0.0914\n",
      "Epoch: 00052, Loss: 0.0911\n",
      "Epoch: 00053, Loss: 0.0894\n",
      "Epoch: 00054, Loss: 0.0894\n",
      "Epoch: 00055, Loss: 0.0878\n",
      "Epoch: 00056, Loss: 0.0874\n",
      "Epoch: 00057, Loss: 0.0889\n",
      "Epoch: 00058, Loss: 0.0844\n",
      "Epoch: 00059, Loss: 0.0849\n",
      "Epoch: 00060, Loss: 0.0834\n",
      "Epoch: 00061, Loss: 0.0829\n",
      "Epoch: 00062, Loss: 0.0826\n",
      "Epoch: 00063, Loss: 0.0820\n",
      "Epoch: 00064, Loss: 0.0812\n",
      "Epoch: 00065, Loss: 0.0804\n",
      "Epoch: 00066, Loss: 0.0811\n",
      "Epoch: 00067, Loss: 0.0808\n",
      "Epoch: 00068, Loss: 0.0804\n",
      "Epoch: 00069, Loss: 0.0793\n",
      "Epoch: 00070, Loss: 0.0774\n",
      "Epoch: 00071, Loss: 0.0778\n",
      "Epoch: 00072, Loss: 0.0760\n",
      "Epoch: 00073, Loss: 0.0772\n",
      "Epoch: 00074, Loss: 0.0750\n",
      "Epoch: 00075, Loss: 0.0772\n",
      "Epoch: 00076, Loss: 0.0750\n",
      "Epoch: 00077, Loss: 0.0779\n",
      "Epoch: 00078, Loss: 0.0726\n",
      "Epoch: 00079, Loss: 0.0737\n",
      "Epoch: 00080, Loss: 0.0726\n",
      "Epoch: 00081, Loss: 0.0745\n",
      "Epoch: 00082, Loss: 0.0721\n",
      "Epoch: 00083, Loss: 0.0732\n",
      "Epoch: 00084, Loss: 0.0705\n",
      "Epoch: 00085, Loss: 0.0705\n",
      "Epoch: 00086, Loss: 0.0710\n",
      "Epoch: 00087, Loss: 0.0687\n",
      "Epoch: 00088, Loss: 0.0705\n",
      "Epoch: 00089, Loss: 0.0702\n",
      "Epoch: 00090, Loss: 0.0697\n",
      "Epoch: 00091, Loss: 0.0677\n",
      "Epoch: 00092, Loss: 0.0688\n",
      "Epoch: 00093, Loss: 0.0679\n",
      "Epoch: 00094, Loss: 0.0690\n",
      "Epoch: 00095, Loss: 0.0655\n",
      "Epoch: 00096, Loss: 0.0673\n",
      "Epoch: 00097, Loss: 0.0684\n",
      "Epoch: 00098, Loss: 0.0690\n",
      "Epoch: 00099, Loss: 0.0694\n",
      "Epoch: 00100, Loss: 0.0671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17535/17535 [02:39<00:00, 109.92it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20466/20466 [03:06<00:00, 109.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val MRR: 0.2053, Test MRR: 0.2024\n",
      "Epoch: 00101, Loss: 0.0674\n",
      "Epoch: 00102, Loss: 0.0710\n",
      "Epoch: 00103, Loss: 0.0656\n",
      "Epoch: 00104, Loss: 0.0684\n",
      "Epoch: 00105, Loss: 0.0665\n",
      "Epoch: 00106, Loss: 0.0683\n",
      "Epoch: 00107, Loss: 0.0648\n",
      "Epoch: 00108, Loss: 0.0677\n",
      "Epoch: 00109, Loss: 0.0669\n",
      "Epoch: 00110, Loss: 0.0651\n",
      "Epoch: 00111, Loss: 0.0633\n",
      "Epoch: 00112, Loss: 0.0622\n",
      "Epoch: 00113, Loss: 0.0616\n",
      "Epoch: 00114, Loss: 0.0632\n",
      "Epoch: 00115, Loss: 0.0618\n",
      "Epoch: 00116, Loss: 0.0614\n",
      "Epoch: 00117, Loss: 0.0621\n",
      "Epoch: 00118, Loss: 0.0621\n",
      "Epoch: 00119, Loss: 0.0627\n",
      "Epoch: 00120, Loss: 0.0624\n",
      "Epoch: 00121, Loss: 0.0614\n",
      "Epoch: 00122, Loss: 0.0612\n",
      "Epoch: 00123, Loss: 0.0606\n",
      "Epoch: 00124, Loss: 0.0594\n",
      "Epoch: 00125, Loss: 0.0605\n",
      "Epoch: 00126, Loss: 0.0589\n",
      "Epoch: 00127, Loss: 0.0587\n",
      "Epoch: 00128, Loss: 0.0608\n",
      "Epoch: 00129, Loss: 0.0587\n",
      "Epoch: 00130, Loss: 0.0594\n",
      "Epoch: 00131, Loss: 0.0590\n",
      "Epoch: 00132, Loss: 0.0583\n",
      "Epoch: 00133, Loss: 0.0591\n",
      "Epoch: 00134, Loss: 0.0594\n",
      "Epoch: 00135, Loss: 0.0600\n",
      "Epoch: 00136, Loss: 0.0571\n",
      "Epoch: 00137, Loss: 0.0584\n",
      "Epoch: 00138, Loss: 0.0583\n",
      "Epoch: 00139, Loss: 0.0575\n",
      "Epoch: 00140, Loss: 0.0572\n",
      "Epoch: 00141, Loss: 0.0570\n",
      "Epoch: 00142, Loss: 0.0570\n",
      "Epoch: 00143, Loss: 0.0575\n",
      "Epoch: 00144, Loss: 0.0564\n",
      "Epoch: 00145, Loss: 0.0558\n",
      "Epoch: 00146, Loss: 0.0555\n",
      "Epoch: 00147, Loss: 0.0554\n",
      "Epoch: 00148, Loss: 0.0559\n",
      "Epoch: 00149, Loss: 0.0560\n",
      "Epoch: 00150, Loss: 0.0561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17535/17535 [02:42<00:00, 108.10it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20466/20466 [03:10<00:00, 107.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val MRR: 0.2179, Test MRR: 0.2140\n",
      "Epoch: 00151, Loss: 0.0550\n",
      "Epoch: 00152, Loss: 0.0552\n",
      "Epoch: 00153, Loss: 0.0546\n",
      "Epoch: 00154, Loss: 0.0538\n",
      "Epoch: 00155, Loss: 0.0540\n",
      "Epoch: 00156, Loss: 0.0538\n",
      "Epoch: 00157, Loss: 0.0552\n",
      "Epoch: 00158, Loss: 0.0546\n",
      "Epoch: 00159, Loss: 0.0548\n",
      "Epoch: 00160, Loss: 0.0535\n",
      "Epoch: 00161, Loss: 0.0554\n",
      "Epoch: 00162, Loss: 0.0530\n",
      "Epoch: 00163, Loss: 0.0546\n",
      "Epoch: 00164, Loss: 0.0541\n",
      "Epoch: 00165, Loss: 0.0556\n",
      "Epoch: 00166, Loss: 0.0534\n",
      "Epoch: 00167, Loss: 0.0543\n",
      "Epoch: 00168, Loss: 0.0532\n",
      "Epoch: 00169, Loss: 0.0539\n",
      "Epoch: 00170, Loss: 0.0524\n",
      "Epoch: 00171, Loss: 0.0537\n",
      "Epoch: 00172, Loss: 0.0534\n",
      "Epoch: 00173, Loss: 0.0530\n",
      "Epoch: 00174, Loss: 0.0529\n",
      "Epoch: 00175, Loss: 0.0532\n",
      "Epoch: 00176, Loss: 0.0527\n",
      "Epoch: 00177, Loss: 0.0518\n",
      "Epoch: 00178, Loss: 0.0522\n",
      "Epoch: 00179, Loss: 0.0522\n",
      "Epoch: 00180, Loss: 0.0529\n",
      "Epoch: 00181, Loss: 0.0510\n",
      "Epoch: 00182, Loss: 0.0519\n",
      "Epoch: 00183, Loss: 0.0511\n",
      "Epoch: 00184, Loss: 0.0511\n",
      "Epoch: 00185, Loss: 0.0518\n",
      "Epoch: 00186, Loss: 0.0519\n",
      "Epoch: 00187, Loss: 0.0517\n",
      "Epoch: 00188, Loss: 0.0515\n",
      "Epoch: 00189, Loss: 0.0514\n",
      "Epoch: 00190, Loss: 0.0521\n",
      "Epoch: 00191, Loss: 0.0514\n",
      "Epoch: 00192, Loss: 0.0520\n",
      "Epoch: 00193, Loss: 0.0504\n",
      "Epoch: 00194, Loss: 0.0516\n",
      "Epoch: 00195, Loss: 0.0503\n",
      "Epoch: 00196, Loss: 0.0509\n",
      "Epoch: 00197, Loss: 0.0497\n",
      "Epoch: 00198, Loss: 0.0506\n",
      "Epoch: 00199, Loss: 0.0514\n",
      "Epoch: 00200, Loss: 0.0502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17535/17535 [02:42<00:00, 107.75it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20466/20466 [03:11<00:00, 107.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val MRR: 0.2316, Test MRR: 0.2258\n",
      "Epoch: 00201, Loss: 0.0519\n",
      "Epoch: 00202, Loss: 0.0502\n",
      "Epoch: 00203, Loss: 0.0504\n",
      "Epoch: 00204, Loss: 0.0515\n",
      "Epoch: 00205, Loss: 0.0496\n",
      "Epoch: 00206, Loss: 0.0511\n",
      "Epoch: 00207, Loss: 0.0499\n",
      "Epoch: 00208, Loss: 0.0498\n",
      "Epoch: 00209, Loss: 0.0489\n",
      "Epoch: 00210, Loss: 0.0508\n",
      "Epoch: 00211, Loss: 0.0497\n",
      "Epoch: 00212, Loss: 0.0495\n",
      "Epoch: 00213, Loss: 0.0499\n",
      "Epoch: 00214, Loss: 0.0499\n",
      "Epoch: 00215, Loss: 0.0490\n",
      "Epoch: 00216, Loss: 0.0497\n",
      "Epoch: 00217, Loss: 0.0484\n",
      "Epoch: 00218, Loss: 0.0488\n",
      "Epoch: 00219, Loss: 0.0488\n",
      "Epoch: 00220, Loss: 0.0492\n",
      "Epoch: 00221, Loss: 0.0475\n",
      "Epoch: 00222, Loss: 0.0502\n",
      "Epoch: 00223, Loss: 0.0473\n",
      "Epoch: 00224, Loss: 0.0492\n",
      "Epoch: 00225, Loss: 0.0483\n",
      "Epoch: 00226, Loss: 0.0477\n",
      "Epoch: 00227, Loss: 0.0472\n",
      "Epoch: 00228, Loss: 0.0481\n",
      "Epoch: 00229, Loss: 0.0479\n",
      "Epoch: 00230, Loss: 0.0484\n",
      "Epoch: 00231, Loss: 0.0472\n",
      "Epoch: 00232, Loss: 0.0474\n",
      "Epoch: 00233, Loss: 0.0466\n",
      "Epoch: 00234, Loss: 0.0474\n",
      "Epoch: 00235, Loss: 0.0482\n",
      "Epoch: 00236, Loss: 0.0475\n",
      "Epoch: 00237, Loss: 0.0481\n",
      "Epoch: 00238, Loss: 0.0466\n",
      "Epoch: 00239, Loss: 0.0467\n",
      "Epoch: 00240, Loss: 0.0472\n",
      "Epoch: 00241, Loss: 0.0469\n",
      "Epoch: 00242, Loss: 0.0478\n",
      "Epoch: 00243, Loss: 0.0480\n",
      "Epoch: 00244, Loss: 0.0479\n",
      "Epoch: 00245, Loss: 0.0468\n",
      "Epoch: 00246, Loss: 0.0474\n",
      "Epoch: 00247, Loss: 0.0471\n",
      "Epoch: 00248, Loss: 0.0478\n",
      "Epoch: 00249, Loss: 0.0475\n",
      "Epoch: 00250, Loss: 0.0479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17535/17535 [02:43<00:00, 106.94it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20466/20466 [03:13<00:00, 105.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val MRR: 0.2285, Test MRR: 0.2198\n",
      "Epoch: 00251, Loss: 0.0459\n",
      "Epoch: 00252, Loss: 0.0461\n",
      "Epoch: 00253, Loss: 0.0458\n",
      "Epoch: 00254, Loss: 0.0459\n",
      "Epoch: 00255, Loss: 0.0460\n",
      "Epoch: 00256, Loss: 0.0466\n",
      "Epoch: 00257, Loss: 0.0460\n",
      "Epoch: 00258, Loss: 0.0462\n",
      "Epoch: 00259, Loss: 0.0464\n",
      "Epoch: 00260, Loss: 0.0458\n",
      "Epoch: 00261, Loss: 0.0466\n",
      "Epoch: 00262, Loss: 0.0457\n",
      "Epoch: 00263, Loss: 0.0461\n",
      "Epoch: 00264, Loss: 0.0473\n",
      "Epoch: 00265, Loss: 0.0458\n",
      "Epoch: 00266, Loss: 0.0457\n",
      "Epoch: 00267, Loss: 0.0462\n",
      "Epoch: 00268, Loss: 0.0457\n",
      "Epoch: 00269, Loss: 0.0465\n",
      "Epoch: 00270, Loss: 0.0452\n",
      "Epoch: 00271, Loss: 0.0469\n",
      "Epoch: 00272, Loss: 0.0468\n",
      "Epoch: 00273, Loss: 0.0454\n",
      "Epoch: 00274, Loss: 0.0466\n",
      "Epoch: 00275, Loss: 0.0455\n",
      "Epoch: 00276, Loss: 0.0456\n",
      "Epoch: 00277, Loss: 0.0458\n",
      "Epoch: 00278, Loss: 0.0450\n",
      "Epoch: 00279, Loss: 0.0452\n",
      "Epoch: 00280, Loss: 0.0468\n",
      "Epoch: 00281, Loss: 0.0458\n",
      "Epoch: 00282, Loss: 0.0450\n",
      "Epoch: 00283, Loss: 0.0461\n",
      "Epoch: 00284, Loss: 0.0465\n",
      "Epoch: 00285, Loss: 0.0473\n",
      "Epoch: 00286, Loss: 0.0468\n",
      "Epoch: 00287, Loss: 0.0450\n",
      "Epoch: 00288, Loss: 0.0461\n",
      "Epoch: 00289, Loss: 0.0453\n",
      "Epoch: 00290, Loss: 0.0446\n",
      "Epoch: 00291, Loss: 0.0464\n",
      "Epoch: 00292, Loss: 0.0454\n",
      "Epoch: 00293, Loss: 0.0455\n",
      "Epoch: 00294, Loss: 0.0459\n",
      "Epoch: 00295, Loss: 0.0442\n",
      "Epoch: 00296, Loss: 0.0453\n",
      "Epoch: 00297, Loss: 0.0456\n",
      "Epoch: 00298, Loss: 0.0452\n",
      "Epoch: 00299, Loss: 0.0451\n",
      "Epoch: 00300, Loss: 0.0449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17535/17535 [02:46<00:00, 105.49it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20466/20466 [03:14<00:00, 105.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val MRR: 0.2316, Test MRR: 0.2263\n",
      "Epoch: 00301, Loss: 0.0442\n",
      "Epoch: 00302, Loss: 0.0447\n",
      "Epoch: 00303, Loss: 0.0446\n",
      "Epoch: 00304, Loss: 0.0443\n",
      "Epoch: 00305, Loss: 0.0459\n",
      "Epoch: 00306, Loss: 0.0442\n",
      "Epoch: 00307, Loss: 0.0434\n",
      "Epoch: 00308, Loss: 0.0443\n",
      "Epoch: 00309, Loss: 0.0448\n",
      "Epoch: 00310, Loss: 0.0450\n",
      "Epoch: 00311, Loss: 0.0434\n",
      "Epoch: 00312, Loss: 0.0443\n",
      "Epoch: 00313, Loss: 0.0447\n",
      "Epoch: 00314, Loss: 0.0442\n",
      "Epoch: 00315, Loss: 0.0447\n",
      "Epoch: 00316, Loss: 0.0444\n",
      "Epoch: 00317, Loss: 0.0442\n",
      "Epoch: 00318, Loss: 0.0436\n",
      "Epoch: 00319, Loss: 0.0440\n",
      "Epoch: 00320, Loss: 0.0441\n",
      "Epoch: 00321, Loss: 0.0437\n",
      "Epoch: 00322, Loss: 0.0448\n",
      "Epoch: 00323, Loss: 0.0424\n",
      "Epoch: 00324, Loss: 0.0441\n",
      "Epoch: 00325, Loss: 0.0439\n",
      "Epoch: 00326, Loss: 0.0432\n",
      "Epoch: 00327, Loss: 0.0436\n",
      "Epoch: 00328, Loss: 0.0428\n",
      "Epoch: 00329, Loss: 0.0438\n",
      "Epoch: 00330, Loss: 0.0442\n",
      "Epoch: 00331, Loss: 0.0443\n",
      "Epoch: 00332, Loss: 0.0439\n",
      "Epoch: 00333, Loss: 0.0436\n",
      "Epoch: 00334, Loss: 0.0439\n",
      "Epoch: 00335, Loss: 0.0433\n",
      "Epoch: 00336, Loss: 0.0433\n",
      "Epoch: 00337, Loss: 0.0438\n",
      "Epoch: 00338, Loss: 0.0435\n",
      "Epoch: 00339, Loss: 0.0428\n",
      "Epoch: 00340, Loss: 0.0430\n",
      "Epoch: 00341, Loss: 0.0426\n",
      "Epoch: 00342, Loss: 0.0434\n",
      "Epoch: 00343, Loss: 0.0440\n",
      "Epoch: 00344, Loss: 0.0431\n",
      "Epoch: 00345, Loss: 0.0435\n",
      "Epoch: 00346, Loss: 0.0432\n",
      "Epoch: 00347, Loss: 0.0433\n",
      "Epoch: 00348, Loss: 0.0433\n",
      "Epoch: 00349, Loss: 0.0430\n",
      "Epoch: 00350, Loss: 0.0434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17535/17535 [03:05<00:00, 94.35it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20466/20466 [03:36<00:00, 94.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val MRR: 0.2307, Test MRR: 0.2226\n",
      "Epoch: 00351, Loss: 0.0436\n",
      "Epoch: 00352, Loss: 0.0432\n",
      "Epoch: 00353, Loss: 0.0430\n",
      "Epoch: 00354, Loss: 0.0428\n",
      "Epoch: 00355, Loss: 0.0429\n",
      "Epoch: 00356, Loss: 0.0425\n",
      "Epoch: 00357, Loss: 0.0431\n",
      "Epoch: 00358, Loss: 0.0430\n",
      "Epoch: 00359, Loss: 0.0429\n",
      "Epoch: 00360, Loss: 0.0433\n",
      "Epoch: 00361, Loss: 0.0430\n",
      "Epoch: 00362, Loss: 0.0429\n",
      "Epoch: 00363, Loss: 0.0425\n",
      "Epoch: 00364, Loss: 0.0427\n",
      "Epoch: 00365, Loss: 0.0431\n",
      "Epoch: 00366, Loss: 0.0425\n",
      "Epoch: 00367, Loss: 0.0420\n",
      "Epoch: 00368, Loss: 0.0429\n",
      "Epoch: 00369, Loss: 0.0427\n",
      "Epoch: 00370, Loss: 0.0422\n",
      "Epoch: 00371, Loss: 0.0435\n",
      "Epoch: 00372, Loss: 0.0421\n",
      "Epoch: 00373, Loss: 0.0427\n",
      "Epoch: 00374, Loss: 0.0425\n",
      "Epoch: 00375, Loss: 0.0425\n",
      "Epoch: 00376, Loss: 0.0425\n",
      "Epoch: 00377, Loss: 0.0417\n",
      "Epoch: 00378, Loss: 0.0430\n",
      "Epoch: 00379, Loss: 0.0433\n",
      "Epoch: 00380, Loss: 0.0422\n",
      "Epoch: 00381, Loss: 0.0417\n",
      "Epoch: 00382, Loss: 0.0430\n",
      "Epoch: 00383, Loss: 0.0411\n",
      "Epoch: 00384, Loss: 0.0428\n",
      "Epoch: 00385, Loss: 0.0434\n",
      "Epoch: 00386, Loss: 0.0423\n",
      "Epoch: 00387, Loss: 0.0414\n",
      "Epoch: 00388, Loss: 0.0415\n",
      "Epoch: 00389, Loss: 0.0419\n",
      "Epoch: 00390, Loss: 0.0421\n",
      "Epoch: 00391, Loss: 0.0413\n",
      "Epoch: 00392, Loss: 0.0415\n",
      "Epoch: 00393, Loss: 0.0417\n",
      "Epoch: 00394, Loss: 0.0418\n",
      "Epoch: 00395, Loss: 0.0421\n",
      "Epoch: 00396, Loss: 0.0414\n",
      "Epoch: 00397, Loss: 0.0415\n",
      "Epoch: 00398, Loss: 0.0424\n",
      "Epoch: 00399, Loss: 0.0425\n",
      "Epoch: 00400, Loss: 0.0415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17535/17535 [02:47<00:00, 104.52it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20466/20466 [03:16<00:00, 104.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val MRR: 0.2340, Test MRR: 0.2276\n",
      "Epoch: 00401, Loss: 0.0405\n",
      "Epoch: 00402, Loss: 0.0416\n",
      "Epoch: 00403, Loss: 0.0410\n",
      "Epoch: 00404, Loss: 0.0412\n",
      "Epoch: 00405, Loss: 0.0417\n",
      "Epoch: 00406, Loss: 0.0408\n",
      "Epoch: 00407, Loss: 0.0421\n",
      "Epoch: 00408, Loss: 0.0413\n",
      "Epoch: 00409, Loss: 0.0415\n",
      "Epoch: 00410, Loss: 0.0413\n",
      "Epoch: 00411, Loss: 0.0417\n",
      "Epoch: 00412, Loss: 0.0419\n",
      "Epoch: 00413, Loss: 0.0408\n",
      "Epoch: 00414, Loss: 0.0422\n",
      "Epoch: 00415, Loss: 0.0411\n",
      "Epoch: 00416, Loss: 0.0415\n",
      "Epoch: 00417, Loss: 0.0409\n",
      "Epoch: 00418, Loss: 0.0413\n",
      "Epoch: 00419, Loss: 0.0419\n",
      "Epoch: 00420, Loss: 0.0408\n",
      "Epoch: 00421, Loss: 0.0409\n",
      "Epoch: 00422, Loss: 0.0414\n",
      "Epoch: 00423, Loss: 0.0417\n",
      "Epoch: 00424, Loss: 0.0419\n",
      "Epoch: 00425, Loss: 0.0411\n",
      "Epoch: 00426, Loss: 0.0415\n",
      "Epoch: 00427, Loss: 0.0420\n",
      "Epoch: 00428, Loss: 0.0425\n",
      "Epoch: 00429, Loss: 0.0412\n",
      "Epoch: 00430, Loss: 0.0420\n",
      "Epoch: 00431, Loss: 0.0424\n",
      "Epoch: 00432, Loss: 0.0408\n",
      "Epoch: 00433, Loss: 0.0410\n",
      "Epoch: 00434, Loss: 0.0428\n",
      "Epoch: 00435, Loss: 0.0418\n",
      "Epoch: 00436, Loss: 0.0417\n",
      "Epoch: 00437, Loss: 0.0410\n",
      "Epoch: 00438, Loss: 0.0413\n",
      "Epoch: 00439, Loss: 0.0417\n",
      "Epoch: 00440, Loss: 0.0401\n",
      "Epoch: 00441, Loss: 0.0412\n",
      "Epoch: 00442, Loss: 0.0412\n",
      "Epoch: 00443, Loss: 0.0417\n",
      "Epoch: 00444, Loss: 0.0407\n",
      "Epoch: 00445, Loss: 0.0412\n",
      "Epoch: 00446, Loss: 0.0415\n",
      "Epoch: 00447, Loss: 0.0407\n",
      "Epoch: 00448, Loss: 0.0411\n",
      "Epoch: 00449, Loss: 0.0405\n",
      "Epoch: 00450, Loss: 0.0406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17535/17535 [02:59<00:00, 97.71it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20466/20466 [03:28<00:00, 97.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val MRR: 0.2419, Test MRR: 0.2345\n",
      "Epoch: 00451, Loss: 0.0401\n",
      "Epoch: 00452, Loss: 0.0407\n",
      "Epoch: 00453, Loss: 0.0403\n",
      "Epoch: 00454, Loss: 0.0416\n",
      "Epoch: 00455, Loss: 0.0398\n",
      "Epoch: 00456, Loss: 0.0410\n",
      "Epoch: 00457, Loss: 0.0404\n",
      "Epoch: 00458, Loss: 0.0410\n",
      "Epoch: 00459, Loss: 0.0408\n",
      "Epoch: 00460, Loss: 0.0404\n",
      "Epoch: 00461, Loss: 0.0398\n",
      "Epoch: 00462, Loss: 0.0401\n",
      "Epoch: 00463, Loss: 0.0407\n",
      "Epoch: 00464, Loss: 0.0397\n",
      "Epoch: 00465, Loss: 0.0409\n",
      "Epoch: 00466, Loss: 0.0404\n",
      "Epoch: 00467, Loss: 0.0396\n",
      "Epoch: 00468, Loss: 0.0396\n",
      "Epoch: 00469, Loss: 0.0390\n",
      "Epoch: 00470, Loss: 0.0407\n",
      "Epoch: 00471, Loss: 0.0407\n",
      "Epoch: 00472, Loss: 0.0400\n",
      "Epoch: 00473, Loss: 0.0399\n",
      "Epoch: 00474, Loss: 0.0393\n",
      "Epoch: 00475, Loss: 0.0410\n",
      "Epoch: 00476, Loss: 0.0402\n",
      "Epoch: 00477, Loss: 0.0398\n",
      "Epoch: 00478, Loss: 0.0400\n",
      "Epoch: 00479, Loss: 0.0399\n",
      "Epoch: 00480, Loss: 0.0396\n",
      "Epoch: 00481, Loss: 0.0398\n",
      "Epoch: 00482, Loss: 0.0401\n",
      "Epoch: 00483, Loss: 0.0390\n",
      "Epoch: 00484, Loss: 0.0398\n",
      "Epoch: 00485, Loss: 0.0393\n",
      "Epoch: 00486, Loss: 0.0403\n",
      "Epoch: 00487, Loss: 0.0395\n",
      "Epoch: 00488, Loss: 0.0401\n",
      "Epoch: 00489, Loss: 0.0401\n",
      "Epoch: 00490, Loss: 0.0398\n",
      "Epoch: 00491, Loss: 0.0391\n",
      "Epoch: 00492, Loss: 0.0391\n",
      "Epoch: 00493, Loss: 0.0386\n",
      "Epoch: 00494, Loss: 0.0396\n",
      "Epoch: 00495, Loss: 0.0393\n",
      "Epoch: 00496, Loss: 0.0395\n",
      "Epoch: 00497, Loss: 0.0389\n",
      "Epoch: 00498, Loss: 0.0388\n",
      "Epoch: 00499, Loss: 0.0389\n",
      "Epoch: 00500, Loss: 0.0396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17535/17535 [02:49<00:00, 103.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20466/20466 [03:18<00:00, 103.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val MRR: 0.2345, Test MRR: 0.2282\n",
      "Epoch: 00501, Loss: 0.0395\n",
      "Epoch: 00502, Loss: 0.0397\n",
      "Epoch: 00503, Loss: 0.0398\n",
      "Epoch: 00504, Loss: 0.0398\n",
      "Epoch: 00505, Loss: 0.0396\n",
      "Epoch: 00506, Loss: 0.0390\n",
      "Epoch: 00507, Loss: 0.0394\n",
      "Epoch: 00508, Loss: 0.0391\n",
      "Epoch: 00509, Loss: 0.0386\n",
      "Epoch: 00510, Loss: 0.0397\n",
      "Epoch: 00511, Loss: 0.0398\n",
      "Epoch: 00512, Loss: 0.0394\n",
      "Epoch: 00513, Loss: 0.0405\n",
      "Epoch: 00514, Loss: 0.0394\n",
      "Epoch: 00515, Loss: 0.0398\n",
      "Epoch: 00516, Loss: 0.0397\n",
      "Epoch: 00517, Loss: 0.0395\n",
      "Epoch: 00518, Loss: 0.0388\n",
      "Epoch: 00519, Loss: 0.0399\n",
      "Epoch: 00520, Loss: 0.0393\n",
      "Epoch: 00521, Loss: 0.0387\n",
      "Epoch: 00522, Loss: 0.0390\n",
      "Epoch: 00523, Loss: 0.0393\n",
      "Epoch: 00524, Loss: 0.0403\n",
      "Epoch: 00525, Loss: 0.0389\n",
      "Epoch: 00526, Loss: 0.0400\n",
      "Epoch: 00527, Loss: 0.0402\n",
      "Epoch: 00528, Loss: 0.0386\n",
      "Epoch: 00529, Loss: 0.0393\n",
      "Epoch: 00530, Loss: 0.0398\n",
      "Epoch: 00531, Loss: 0.0387\n",
      "Epoch: 00532, Loss: 0.0394\n",
      "Epoch: 00533, Loss: 0.0386\n",
      "Epoch: 00534, Loss: 0.0381\n",
      "Epoch: 00535, Loss: 0.0389\n",
      "Epoch: 00536, Loss: 0.0376\n",
      "Epoch: 00537, Loss: 0.0399\n",
      "Epoch: 00538, Loss: 0.0392\n",
      "Epoch: 00539, Loss: 0.0386\n",
      "Epoch: 00540, Loss: 0.0402\n",
      "Epoch: 00541, Loss: 0.0408\n",
      "Epoch: 00542, Loss: 0.0384\n",
      "Epoch: 00543, Loss: 0.0390\n",
      "Epoch: 00544, Loss: 0.0381\n",
      "Epoch: 00545, Loss: 0.0378\n",
      "Epoch: 00546, Loss: 0.0391\n",
      "Epoch: 00547, Loss: 0.0394\n",
      "Epoch: 00548, Loss: 0.0391\n",
      "Epoch: 00549, Loss: 0.0384\n",
      "Epoch: 00550, Loss: 0.0381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17535/17535 [03:00<00:00, 97.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20466/20466 [03:31<00:00, 96.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val MRR: 0.2384, Test MRR: 0.2322\n",
      "Epoch: 00551, Loss: 0.0393\n",
      "Epoch: 00552, Loss: 0.0381\n",
      "Epoch: 00553, Loss: 0.0389\n",
      "Epoch: 00554, Loss: 0.0387\n",
      "Epoch: 00555, Loss: 0.0387\n",
      "Epoch: 00556, Loss: 0.0391\n",
      "Epoch: 00557, Loss: 0.0384\n",
      "Epoch: 00558, Loss: 0.0388\n",
      "Epoch: 00559, Loss: 0.0380\n",
      "Epoch: 00560, Loss: 0.0384\n",
      "Epoch: 00561, Loss: 0.0378\n",
      "Epoch: 00562, Loss: 0.0376\n",
      "Epoch: 00563, Loss: 0.0384\n",
      "Epoch: 00564, Loss: 0.0383\n",
      "Epoch: 00565, Loss: 0.0388\n",
      "Epoch: 00566, Loss: 0.0394\n",
      "Epoch: 00567, Loss: 0.0384\n",
      "Epoch: 00568, Loss: 0.0396\n",
      "Epoch: 00569, Loss: 0.0393\n",
      "Epoch: 00570, Loss: 0.0393\n",
      "Epoch: 00571, Loss: 0.0381\n",
      "Epoch: 00572, Loss: 0.0387\n",
      "Epoch: 00573, Loss: 0.0393\n",
      "Epoch: 00574, Loss: 0.0389\n",
      "Epoch: 00575, Loss: 0.0388\n",
      "Epoch: 00576, Loss: 0.0381\n",
      "Epoch: 00577, Loss: 0.0392\n",
      "Epoch: 00578, Loss: 0.0388\n",
      "Epoch: 00579, Loss: 0.0380\n",
      "Epoch: 00580, Loss: 0.0393\n",
      "Epoch: 00581, Loss: 0.0377\n",
      "Epoch: 00582, Loss: 0.0391\n",
      "Epoch: 00583, Loss: 0.0406\n",
      "Epoch: 00584, Loss: 0.0383\n",
      "Epoch: 00585, Loss: 0.0395\n",
      "Epoch: 00586, Loss: 0.0391\n",
      "Epoch: 00587, Loss: 0.0391\n",
      "Epoch: 00588, Loss: 0.0385\n",
      "Epoch: 00589, Loss: 0.0389\n",
      "Epoch: 00590, Loss: 0.0396\n",
      "Epoch: 00591, Loss: 0.0393\n",
      "Epoch: 00592, Loss: 0.0386\n",
      "Epoch: 00593, Loss: 0.0391\n",
      "Epoch: 00594, Loss: 0.0388\n",
      "Epoch: 00595, Loss: 0.0395\n",
      "Epoch: 00596, Loss: 0.0390\n",
      "Epoch: 00597, Loss: 0.0392\n",
      "Epoch: 00598, Loss: 0.0391\n",
      "Epoch: 00599, Loss: 0.0380\n",
      "Epoch: 00600, Loss: 0.0378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17535/17535 [02:59<00:00, 97.88it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20466/20466 [03:18<00:00, 103.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val MRR: 0.2381, Test MRR: 0.2295\n",
      "Epoch: 00601, Loss: 0.0385\n",
      "Epoch: 00602, Loss: 0.0393\n",
      "Epoch: 00603, Loss: 0.0382\n",
      "Epoch: 00604, Loss: 0.0386\n",
      "Epoch: 00605, Loss: 0.0387\n",
      "Epoch: 00606, Loss: 0.0388\n",
      "Epoch: 00607, Loss: 0.0384\n",
      "Epoch: 00608, Loss: 0.0381\n",
      "Epoch: 00609, Loss: 0.0386\n",
      "Epoch: 00610, Loss: 0.0382\n",
      "Epoch: 00611, Loss: 0.0384\n",
      "Epoch: 00612, Loss: 0.0377\n",
      "Epoch: 00613, Loss: 0.0387\n",
      "Epoch: 00614, Loss: 0.0397\n",
      "Epoch: 00615, Loss: 0.0378\n",
      "Epoch: 00616, Loss: 0.0387\n",
      "Epoch: 00617, Loss: 0.0377\n",
      "Epoch: 00618, Loss: 0.0396\n",
      "Epoch: 00619, Loss: 0.0385\n",
      "Epoch: 00620, Loss: 0.0379\n",
      "Epoch: 00621, Loss: 0.0383\n",
      "Epoch: 00622, Loss: 0.0383\n",
      "Epoch: 00623, Loss: 0.0380\n",
      "Epoch: 00624, Loss: 0.0381\n",
      "Epoch: 00625, Loss: 0.0379\n",
      "Epoch: 00626, Loss: 0.0386\n",
      "Epoch: 00627, Loss: 0.0382\n",
      "Epoch: 00628, Loss: 0.0388\n",
      "Epoch: 00629, Loss: 0.0379\n",
      "Epoch: 00630, Loss: 0.0384\n",
      "Epoch: 00631, Loss: 0.0387\n",
      "Epoch: 00632, Loss: 0.0379\n",
      "Epoch: 00633, Loss: 0.0378\n",
      "Epoch: 00634, Loss: 0.0374\n",
      "Epoch: 00635, Loss: 0.0375\n",
      "Epoch: 00636, Loss: 0.0393\n",
      "Epoch: 00637, Loss: 0.0382\n",
      "Epoch: 00638, Loss: 0.0379\n",
      "Epoch: 00639, Loss: 0.0380\n",
      "Epoch: 00640, Loss: 0.0370\n",
      "Epoch: 00641, Loss: 0.0379\n",
      "Epoch: 00642, Loss: 0.0385\n",
      "Epoch: 00643, Loss: 0.0379\n",
      "Epoch: 00644, Loss: 0.0378\n",
      "Epoch: 00645, Loss: 0.0367\n",
      "Epoch: 00646, Loss: 0.0375\n",
      "Epoch: 00647, Loss: 0.0381\n",
      "Epoch: 00648, Loss: 0.0383\n",
      "Epoch: 00649, Loss: 0.0380\n",
      "Epoch: 00650, Loss: 0.0380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17535/17535 [02:55<00:00, 99.75it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20466/20466 [03:29<00:00, 97.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val MRR: 0.2345, Test MRR: 0.2293\n",
      "Epoch: 00651, Loss: 0.0378\n",
      "Epoch: 00652, Loss: 0.0372\n",
      "Epoch: 00653, Loss: 0.0375\n",
      "Epoch: 00654, Loss: 0.0379\n",
      "Epoch: 00655, Loss: 0.0367\n",
      "Epoch: 00656, Loss: 0.0380\n",
      "Epoch: 00657, Loss: 0.0375\n",
      "Epoch: 00658, Loss: 0.0367\n",
      "Epoch: 00659, Loss: 0.0375\n",
      "Epoch: 00660, Loss: 0.0362\n",
      "Epoch: 00661, Loss: 0.0381\n",
      "Epoch: 00662, Loss: 0.0374\n",
      "Epoch: 00663, Loss: 0.0389\n",
      "Epoch: 00664, Loss: 0.0375\n",
      "Epoch: 00665, Loss: 0.0372\n",
      "Epoch: 00666, Loss: 0.0384\n",
      "Epoch: 00667, Loss: 0.0371\n",
      "Epoch: 00668, Loss: 0.0370\n",
      "Epoch: 00669, Loss: 0.0371\n",
      "Epoch: 00670, Loss: 0.0379\n",
      "Epoch: 00671, Loss: 0.0379\n",
      "Epoch: 00672, Loss: 0.0374\n",
      "Epoch: 00673, Loss: 0.0368\n",
      "Epoch: 00674, Loss: 0.0372\n",
      "Epoch: 00675, Loss: 0.0378\n",
      "Epoch: 00676, Loss: 0.0380\n",
      "Epoch: 00677, Loss: 0.0383\n",
      "Epoch: 00678, Loss: 0.0383\n",
      "Epoch: 00679, Loss: 0.0385\n",
      "Epoch: 00680, Loss: 0.0383\n",
      "Epoch: 00681, Loss: 0.0385\n",
      "Epoch: 00682, Loss: 0.0385\n",
      "Epoch: 00683, Loss: 0.0379\n",
      "Epoch: 00684, Loss: 0.0377\n",
      "Epoch: 00685, Loss: 0.0370\n",
      "Epoch: 00686, Loss: 0.0373\n",
      "Epoch: 00687, Loss: 0.0380\n",
      "Epoch: 00688, Loss: 0.0378\n",
      "Epoch: 00689, Loss: 0.0382\n",
      "Epoch: 00690, Loss: 0.0381\n",
      "Epoch: 00691, Loss: 0.0378\n",
      "Epoch: 00692, Loss: 0.0374\n",
      "Epoch: 00693, Loss: 0.0367\n",
      "Epoch: 00694, Loss: 0.0373\n",
      "Epoch: 00695, Loss: 0.0380\n",
      "Epoch: 00696, Loss: 0.0378\n",
      "Epoch: 00697, Loss: 0.0363\n",
      "Epoch: 00698, Loss: 0.0377\n",
      "Epoch: 00699, Loss: 0.0384\n",
      "Epoch: 00700, Loss: 0.0384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17535/17535 [02:46<00:00, 105.61it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20466/20466 [03:15<00:00, 104.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val MRR: 0.2381, Test MRR: 0.2323\n",
      "Epoch: 00701, Loss: 0.0376\n",
      "Epoch: 00702, Loss: 0.0399\n",
      "Epoch: 00703, Loss: 0.0374\n",
      "Epoch: 00704, Loss: 0.0385\n",
      "Epoch: 00705, Loss: 0.0369\n",
      "Epoch: 00706, Loss: 0.0379\n",
      "Epoch: 00707, Loss: 0.0383\n",
      "Epoch: 00708, Loss: 0.0377\n",
      "Epoch: 00709, Loss: 0.0370\n",
      "Epoch: 00710, Loss: 0.0371\n",
      "Epoch: 00711, Loss: 0.0359\n",
      "Epoch: 00712, Loss: 0.0376\n",
      "Epoch: 00713, Loss: 0.0371\n",
      "Epoch: 00714, Loss: 0.0372\n",
      "Epoch: 00715, Loss: 0.0380\n",
      "Epoch: 00716, Loss: 0.0370\n",
      "Epoch: 00717, Loss: 0.0363\n",
      "Epoch: 00718, Loss: 0.0384\n",
      "Epoch: 00719, Loss: 0.0379\n",
      "Epoch: 00720, Loss: 0.0368\n",
      "Epoch: 00721, Loss: 0.0371\n",
      "Epoch: 00722, Loss: 0.0373\n",
      "Epoch: 00723, Loss: 0.0375\n",
      "Epoch: 00724, Loss: 0.0372\n",
      "Epoch: 00725, Loss: 0.0373\n",
      "Epoch: 00726, Loss: 0.0371\n",
      "Epoch: 00727, Loss: 0.0379\n",
      "Epoch: 00728, Loss: 0.0370\n",
      "Epoch: 00729, Loss: 0.0370\n",
      "Epoch: 00730, Loss: 0.0367\n",
      "Epoch: 00731, Loss: 0.0379\n",
      "Epoch: 00732, Loss: 0.0376\n",
      "Epoch: 00733, Loss: 0.0373\n",
      "Epoch: 00734, Loss: 0.0361\n",
      "Epoch: 00735, Loss: 0.0372\n",
      "Epoch: 00736, Loss: 0.0369\n",
      "Epoch: 00737, Loss: 0.0365\n",
      "Epoch: 00738, Loss: 0.0364\n",
      "Epoch: 00739, Loss: 0.0365\n",
      "Epoch: 00740, Loss: 0.0367\n",
      "Epoch: 00741, Loss: 0.0372\n",
      "Epoch: 00742, Loss: 0.0370\n",
      "Epoch: 00743, Loss: 0.0368\n",
      "Epoch: 00744, Loss: 0.0364\n",
      "Epoch: 00745, Loss: 0.0370\n",
      "Epoch: 00746, Loss: 0.0373\n",
      "Epoch: 00747, Loss: 0.0365\n",
      "Epoch: 00748, Loss: 0.0376\n",
      "Epoch: 00749, Loss: 0.0371\n",
      "Epoch: 00750, Loss: 0.0370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17535/17535 [02:56<00:00, 99.30it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20466/20466 [03:14<00:00, 105.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val MRR: 0.2324, Test MRR: 0.2273\n",
      "Epoch: 00751, Loss: 0.0370\n",
      "Epoch: 00752, Loss: 0.0367\n",
      "Epoch: 00753, Loss: 0.0366\n",
      "Epoch: 00754, Loss: 0.0362\n",
      "Epoch: 00755, Loss: 0.0373\n",
      "Epoch: 00756, Loss: 0.0372\n",
      "Epoch: 00757, Loss: 0.0360\n",
      "Epoch: 00758, Loss: 0.0370\n",
      "Epoch: 00759, Loss: 0.0363\n",
      "Epoch: 00760, Loss: 0.0379\n",
      "Epoch: 00761, Loss: 0.0368\n",
      "Epoch: 00762, Loss: 0.0372\n",
      "Epoch: 00763, Loss: 0.0365\n",
      "Epoch: 00764, Loss: 0.0375\n",
      "Epoch: 00765, Loss: 0.0369\n",
      "Epoch: 00766, Loss: 0.0369\n",
      "Epoch: 00767, Loss: 0.0366\n",
      "Epoch: 00768, Loss: 0.0374\n",
      "Epoch: 00769, Loss: 0.0364\n",
      "Epoch: 00770, Loss: 0.0375\n",
      "Epoch: 00771, Loss: 0.0368\n",
      "Epoch: 00772, Loss: 0.0372\n",
      "Epoch: 00773, Loss: 0.0365\n",
      "Epoch: 00774, Loss: 0.0371\n",
      "Epoch: 00775, Loss: 0.0363\n",
      "Epoch: 00776, Loss: 0.0370\n",
      "Epoch: 00777, Loss: 0.0365\n",
      "Epoch: 00778, Loss: 0.0369\n",
      "Epoch: 00779, Loss: 0.0362\n",
      "Epoch: 00780, Loss: 0.0371\n",
      "Epoch: 00781, Loss: 0.0365\n",
      "Epoch: 00782, Loss: 0.0358\n",
      "Epoch: 00783, Loss: 0.0367\n",
      "Epoch: 00784, Loss: 0.0373\n",
      "Epoch: 00785, Loss: 0.0365\n",
      "Epoch: 00786, Loss: 0.0360\n",
      "Epoch: 00787, Loss: 0.0366\n",
      "Epoch: 00788, Loss: 0.0367\n",
      "Epoch: 00789, Loss: 0.0360\n",
      "Epoch: 00790, Loss: 0.0367\n",
      "Epoch: 00791, Loss: 0.0359\n",
      "Epoch: 00792, Loss: 0.0370\n",
      "Epoch: 00793, Loss: 0.0360\n",
      "Epoch: 00794, Loss: 0.0368\n",
      "Epoch: 00795, Loss: 0.0364\n",
      "Epoch: 00796, Loss: 0.0362\n",
      "Epoch: 00797, Loss: 0.0368\n",
      "Epoch: 00798, Loss: 0.0362\n",
      "Epoch: 00799, Loss: 0.0363\n",
      "Epoch: 00800, Loss: 0.0361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17535/17535 [02:54<00:00, 100.65it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20466/20466 [03:27<00:00, 98.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val MRR: 0.2373, Test MRR: 0.2315\n",
      "Epoch: 00801, Loss: 0.0364\n",
      "Epoch: 00802, Loss: 0.0367\n",
      "Epoch: 00803, Loss: 0.0370\n",
      "Epoch: 00804, Loss: 0.0376\n",
      "Epoch: 00805, Loss: 0.0377\n",
      "Epoch: 00806, Loss: 0.0367\n",
      "Epoch: 00807, Loss: 0.0370\n",
      "Epoch: 00808, Loss: 0.0359\n",
      "Epoch: 00809, Loss: 0.0379\n",
      "Epoch: 00810, Loss: 0.0372\n",
      "Epoch: 00811, Loss: 0.0370\n",
      "Epoch: 00812, Loss: 0.0369\n",
      "Epoch: 00813, Loss: 0.0376\n",
      "Epoch: 00814, Loss: 0.0364\n",
      "Epoch: 00815, Loss: 0.0364\n",
      "Epoch: 00816, Loss: 0.0376\n",
      "Epoch: 00817, Loss: 0.0361\n",
      "Epoch: 00818, Loss: 0.0370\n",
      "Epoch: 00819, Loss: 0.0365\n",
      "Epoch: 00820, Loss: 0.0373\n",
      "Epoch: 00821, Loss: 0.0355\n",
      "Epoch: 00822, Loss: 0.0370\n",
      "Epoch: 00823, Loss: 0.0367\n",
      "Epoch: 00824, Loss: 0.0371\n",
      "Epoch: 00825, Loss: 0.0370\n",
      "Epoch: 00826, Loss: 0.0377\n",
      "Epoch: 00827, Loss: 0.0385\n",
      "Epoch: 00828, Loss: 0.0396\n",
      "Epoch: 00829, Loss: 0.0386\n",
      "Epoch: 00830, Loss: 0.0385\n",
      "Epoch: 00831, Loss: 0.0405\n",
      "Epoch: 00832, Loss: 0.0386\n",
      "Epoch: 00833, Loss: 0.0412\n",
      "Epoch: 00834, Loss: 0.0386\n",
      "Epoch: 00835, Loss: 0.0390\n",
      "Epoch: 00836, Loss: 0.0377\n",
      "Epoch: 00837, Loss: 0.0380\n",
      "Epoch: 00838, Loss: 0.0389\n",
      "Epoch: 00839, Loss: 0.0385\n",
      "Epoch: 00840, Loss: 0.0364\n",
      "Epoch: 00841, Loss: 0.0380\n",
      "Epoch: 00842, Loss: 0.0358\n",
      "Epoch: 00843, Loss: 0.0384\n",
      "Epoch: 00844, Loss: 0.0369\n",
      "Epoch: 00845, Loss: 0.0375\n",
      "Epoch: 00846, Loss: 0.0383\n",
      "Epoch: 00847, Loss: 0.0375\n",
      "Epoch: 00848, Loss: 0.0363\n",
      "Epoch: 00849, Loss: 0.0369\n",
      "Epoch: 00850, Loss: 0.0371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17535/17535 [02:46<00:00, 105.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20466/20466 [05:01<00:00, 67.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val MRR: 0.2320, Test MRR: 0.2264\n",
      "Epoch: 00851, Loss: 0.0373\n",
      "Epoch: 00852, Loss: 0.0376\n",
      "Epoch: 00853, Loss: 0.0369\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m51\u001b[39m, \u001b[38;5;241m1001\u001b[39m):\n\u001b[1;32m      3\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 4\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[2], line 84\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m reg_loss \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m+\u001b[39m model\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mrel_emb\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     82\u001b[0m loss \u001b[38;5;241m=\u001b[39m cross_entropy_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-2\u001b[39m \u001b[38;5;241m*\u001b[39m reg_loss\n\u001b[0;32m---> 84\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.\u001b[39m)\n\u001b[1;32m     86\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/motif-disc/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/motif-disc/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "times = []\n",
    "for epoch in range(51, 1001):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:05d}, Loss: {loss:.4f}')\n",
    "    if (epoch % 50) == 0:\n",
    "        valid_mrr, test_mrr = test()\n",
    "        print(f'Val MRR: {valid_mrr:.4f}, Test MRR: {test_mrr:.4f}')\n",
    "        test_mrr = round(test_mrr.item()*100, 4)\n",
    "        torch.save(model.state_dict(), './mods/rgcn_fbk_epch_'+str(epoch)+'_mrr'+str(test_mrr)+'.pt')\n",
    "        torch.save(optimizer.state_dict(), './mods/rgcn_fbk_optim_epch_'+str(epoch)+'.pt')\n",
    "    times.append(time.time() - start)\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "204f2a0f-5ba3-4c59-85b4-35eefc2bfd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2132"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times = []\n",
    "for epoch in range(51, 1001):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:05d}, Loss: {loss:.4f}')\n",
    "    if (epoch % 50) == 0:\n",
    "        valid_mrr, test_mrr = test()\n",
    "        print(f'Val MRR: {valid_mrr:.4f}, Test MRR: {test_mrr:.4f}')\n",
    "        test_mrr = round(test_mrr.item()*100, 4)\n",
    "        torch.save(model.state_dict(), './mods/rgcn_fbk_epch_'+str(epoch)+'_mrr'+str(test_mrr)+'.pt')\n",
    "        torch.save(optimizer.state_dict(), './mods/rgcn_fbk_optim_epch_'+str(epoch)+'.pt')\n",
    "    times.append(time.time() - start)\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbfea7af-055c-4271-8d90-bb1f8f517113",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mrr = round(test_mrr.item()*100, 4)\n",
    "torch.save(model.state_dict(), './mods/rgcn_fbk_epch_'+str(epoch)+'_mrr'+str(test_mrr)+'.pt')\n",
    "torch.save(optimizer.state_dict(), './mods/rgcn_fbk_optim_epch_'+str(epoch)+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2937bb38-50c0-40ff-924d-5f279e80a74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18245770037174225"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f8e88b-3d45-4a5a-a881-1e3f3e71f900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
